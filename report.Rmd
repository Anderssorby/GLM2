---
title: "Logistic regression and Poisson regression"
author: "Anders Christiansen SÃ¸rby, Edvard Hove"
date: "October 4, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ggplot2")
#install.packages("GGally")
library(ggplot2)
```

# Part 1: Logistic regression

In this part we model the probability of a successful ascent of a mountain using binary regression with a logit link.
Let $y_i$ be the number of successful ascents, and let $n_i$ be the total number of attempts (sum of successful and failed) of the $i$th mountain.
The GLM model is then,

1. Model for response: $Y_i \sim \text{Bin}(n_i, \pi_i) \quad \text{for} \  i = 1,\ldots,113$
2. Linear predictor: $\eta_i=x^T_i\beta$
3. Link function: $\eta_i = \ln \left(\frac{\pi_i}{1-\pi_i}\right)$

where $\mathbf{x}_i$ is $p$ dimensional column vector of covariates for observation $i$, and $\boldsymbol\beta$ is the vector of regression parameters.

## a)

Since we assume independent pairs of $(y_i,\mathbf{x}_i)$, the likelihood as a function can then be written as

\begin{align}
L(\beta) = \prod_{i=1}^n\pi_i^{y_i}(1-\pi_i)^{1-y_i}
\end{align}

the log-likelihood as a function of $\beta$ is then found by taking the logarithm and using the link function to replace $\pi_i$.

\begin{align}
\label{eq:loglik}
\ell(\beta) &= \sum_{i=1}^n y_i \ln \pi_i + (1-y_i) \ln (1-\pi_i) \\
&= \sum_{i=1}^n y_i \ln \left(\frac{\pi_i}{1-\pi_i}\right) + \ln(1-\pi_i) \\
&= \sum_{i=1}^n y_i \mathbf{x}_i \beta - \ln(1+e^{\mathbf{x}_i \beta})
\end{align}


The maximum likelihood estimate of $\beta$ is then found by maximizing $\ell(\beta)$.
For some models this can be expressed on closed form by solving 

\begin{align}
s(\hat \beta) = \frac{\partial \ell(\hat \beta)}{\partial \beta} = 0
\end{align}

In this case the equations above are non-linear, and the maximum is found using numerical methods like Newton-Raphson or Fisher scoring.

## b)

```{r}
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
mount <- read.table(file = filepath, header = TRUE, 
                    col.names = c("height", "prominence", "fail", "success"))
```

```{r}
model1 <- glm(cbind(success, fail) ~ height + prominence, data = mount, family = "binomial")
summary(model1)
```

The model parameters are, as can be seen from the output, $\beta_0 = 13.7, \beta_1=-0.00164$ and $\beta_2=-0.000174$. This means that if the height and prominence is 0, i.e. $x_1=x_2=0$, the odds of ascending the mountain successfully is $\exp\beta_0 \approx 8.82 \cdot 10^5$. These are a very high odds, which makes sense as a mountain with no height nor prominence have already been ascended, resulting in a high odds. Now, when letting $x_1 \neq 0$ and $x_2$ fixed, the odds of successfully climbing a mountain decreases when $x_1$ increases by 1. More specifically, if the height of the mountain increase by 1, i.e. going from $x_1$ to $x_1+1$, the odds of climbing the 1 m lower mountain are multiplied with $\exp\beta_1 = \exp(-0.00164) = 0.9984$. In other words, it gets slightly harder to climb a taller mountain, since the odds are multiplied with a factor $<1$, so this makes sense. The same goes for $x_2$ and $\beta_2$. Letting $x_1$ be fixed and $x_2 \neq 0$, the odds of ascending a mountain decrease when the prominence increases. More specifically, when the prominence is increased by 1, i.e. going from $x_2$ to $x_2+1$, the previous odds of ascending a mountain 1 m lower are multiplied with $\exp \beta_2 = \exp(-0.000174) = 0.9998$. As can be seen, increasing the height has more impact on the success rate of climbing the mountain than the prominence, as $\beta_1 < \beta_2$.

When hypothesis testing whether a single model parameter $\beta_i = 0$ or not, the Wald test and the Z test are equivalent since the Wald statistic then becomes the Z statistic squared, which obviously is $\chi^2$.
According to the Wald, the `prominence` parameter is less significant than the others, but they are still all highly significant.

Calculating a 95 % CI:
```{r}
sds = sqrt(summary(model1)$cov.scaled[2,2])
alpha = 0.05
lower = model1$coefficients[2] - qnorm(1 - alpha/2) * sds
upper = model1$coefficients[2] + qnorm(1 - alpha/2) * sds
cbind(lower, upper)
```


As can be seen, a 95 % CI for the height parameter is $[\hat \beta_L, \hat \beta_H] = [-0.00191, -0.00136]$.

A corresponding CI for $[\exp{\hat \beta_L}, \exp{\hat \beta_H}]$ is  $[0.9981,0.9986]$. 
This means that the when $x_1$ increases by 1, the odds of successfully climbing a 1 m taller mountain is, with 95 % certainty, multiplied with factor lying in the interval $[0.9981,0.9986]$.   

## c)

The saturated model with one estimated parameter per data point is given by

\begin{align}
\tilde{\lambda}_i = y_i.
\end{align}

Combining this with the log-likelihood from equation \eqref{eq:loglik} we can express the deviance as

\begin{align}
D &= -2(\ell(\hat \lambda)-\ell(\tilde{\lambda})) \\
&= 2 \sum_{i=1}^n \left(y_i(\ln y_i - \ln \hat{y}_i)-(y_i-\hat{y}_i)\right).
\end{align}

Since our candidate model includes an intercept the last term will cancel out (this follows directly from $s(\hat \beta ) = 0$). We can then write

\begin{align}
D=2\sum_{i=1}^n y_i \ln \frac{y_i}{\hat{y}_i} = \sum_{i=1}^n D_i,
\end{align}

The deviance residuals are then 

\begin{align}
d_i = \text{sign}(y_i-\hat{y}_i)\sqrt{D_i} =\text{sign}(y_i-\hat{y}_i)\sqrt{2\ln \frac{y_i}{\hat{y}_i}}
\end{align}

```{r residuals, fig.cap="\\label{fig:dresheight} Deviance residuals as a function of height."}
dres <- residuals(model1, type = "deviance")
plotdf <- data.frame(dres = dres, fitted = model1$fitted.values,
                     height=mount$height, prom = mount$prominence)
ggplot(plotdf, aes(x=height, y=dres)) +geom_point() + labs(x ="Height", y="Deviance residuals" )
```

```{r resprominence, fig.cap="\\label{fig:dresprom} Deviance residuals as a function of prominence."}
ggplot(plotdf, aes(x=prom, y=dres)) +geom_point() + labs(x ="Prominence", y="Deviance residuals" )
```

In figures \ref{fig:dresheight} and \ref{fig:dresprom} the deviance residuals are plotted vs `height` and `prominence`, respectively.
There is no obvious trend in either of the plots.
The deviance residuals seem to have mean and variance independent of both covariates.

ASSESSMENT OF MODEL FIT USING MODEL DEVIANCE.

```{r probplot,fig.cap="\\label{fig:probabilities} The probability of a successful ascent as a function of both height and prominence."}
steps <- 100

hmin <- min(plotdf$height)
hmax <- max(plotdf$height)
heights <- seq(hmin,hmax, length.out=steps)

pmin <- min(plotdf$prom)
pmax <- max(plotdf$prom)
proms <- seq(pmin,pmax,length.out=steps)

prob <- function(height, prom, model){
  eta <- c(1,height,prom)%*%model$coefficients
  res <- exp(eta)/(1+exp(eta))
  return(res)
}

probs <- c()
pcoord <- c()
hcoord <- c()
for (i in 1:steps){
  for (j in 1:steps){
    probs <- append(probs, prob(heights[i],proms[j],model1))
    hcoord <- append(hcoord,heights[i])
    pcoord <- append(pcoord,proms[j])
  }
}

probsdf <- data.frame(prob=probs, he=hcoord, pr=pcoord)
ggplot(data = probsdf, aes(x=he, y=pr)) + geom_raster(aes(fill=prob)) + scale_fill_gradientn(colours = terrain.colors(10)) +labs(x="Height",y="Prominence",colour="Probability") 
```

Figure \ref{fig:probabilities} shows how the probability of a successful ascent depends on both covariates.
According to the model the probability decreases monotonously with both `height` and `prominence`, and it is possible to compensate an increase in one covariate by decreasing the other.
As expected the model predicts probabilities close to zero as both covariates increase and close to one as both decrease.

# Part 2: Poisson regression

```{r tippeligaen}
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/eliteserien2018"
tippeliga <- read.table(file = filepath, header = TRUE,
                        colClasses = c("character", "character", "numeric", "numeric"))
head(tippeliga)
```
```{r dataplot}
library(GGally)
#ggpairs(tippeliga)
```

The loglikelihood for Poisson regression is

\begin{align}
\ell(\beta) &= \ln \prod_{i=1}^n \frac{\lambda_i^{y_i}}{y_i!} e^{-\lambda_i} \\
            &= \sum_{i=1}^n\left[y_i \ln(\lambda_i) - \ln(y_i!) - \lambda_i\right]
\end{align}

```{r myglm}
source('myglm.R')
```

## a)

The Pearson $\chi^2$-test is used to test goodness of fit, homogeneity and independece of some data. In this case a test for independence via a contingency table is necessary. 

Independence between the goals of the home and the away team is questionable. It seems natural that if the away team has more goals the home team would have less. 

```{r chisq}

#chisq.test(X, y)
```

## b)


```{r useglm}
model2b <- myglm(yh ~ home + away + ya, data = tippeliga)
summary(model2b)
```

## c)



## d)

```{r}
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/unplayed2018"
eliteserie <- read.table(file = filepath, header = TRUE, colClasses = c("character", 
    "character"))
```

# Source code
## myglm.R
```{r source, code=readLines('myglm.R')}
```